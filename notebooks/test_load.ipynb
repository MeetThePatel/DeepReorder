{
 "cells": [
  {
   "cell_type": "code",
   "id": "5f1d30e34ed8a942",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T02:49:54.737408Z",
     "start_time": "2025-01-30T02:49:52.961397Z"
    }
   },
   "source": [
    "import torch\n",
    "\n",
    "from deep_reorder.models import qwen25"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import error: No module named 'triton'\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ed6ce89fb8f1b6eb"
  },
  {
   "cell_type": "code",
   "id": "a9c45640cdf62b2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T02:50:09.283220Z",
     "start_time": "2025-01-30T02:49:54.740696Z"
    }
   },
   "source": [
    "model = qwen25.load_model(weights_path=\"../build/Qwen2_5-3B.safetensors\", device='mps')"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "e70d55e3a04b4490",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T02:50:09.590883Z",
     "start_time": "2025-01-30T02:50:09.585517Z"
    }
   },
   "source": [
    "def get_model_size(model):\n",
    "    param_size = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "    return param_size / 1024 ** 3  # Size in GB"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "4850a6d9918e4bc6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T02:50:09.614961Z",
     "start_time": "2025-01-30T02:50:09.609989Z"
    }
   },
   "source": [
    "get_model_size(model)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.748008728027344"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "b8615030f1f3ef3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T02:50:09.869868Z",
     "start_time": "2025-01-30T02:50:09.717017Z"
    }
   },
   "source": [
    "tokenizer = qwen25.load_tokenizer('../build/vocab.json', '../build/merges.txt', '../build/tokenizer.json')"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "9fbec082a72a39fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T02:50:09.878035Z",
     "start_time": "2025-01-30T02:50:09.876130Z"
    }
   },
   "source": [
    "params = qwen25.Qwen25GeneratorParams(max_length=15)"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "36098f79aa42531f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T02:50:10.838350Z",
     "start_time": "2025-01-30T02:50:09.883217Z"
    }
   },
   "source": [
    "generator = qwen25.Qwen25Generator(model, tokenizer, params, device='mps')"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "effc88b19c517358",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T02:50:10.845851Z",
     "start_time": "2025-01-30T02:50:10.844300Z"
    }
   },
   "source": [
    "prompt = \"Who was the first president of the United States of\""
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T02:50:11.721100Z",
     "start_time": "2025-01-30T02:50:10.851066Z"
    }
   },
   "cell_type": "code",
   "source": "generator.generate(prompt)",
   "id": "43ac38974063a7e0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Who was the first president of the United States of precisely precisely precisely precisely precisely'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "e1071570cfe9e7f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T03:09:07.041041Z",
     "start_time": "2025-01-30T03:09:06.969905Z"
    }
   },
   "source": [
    "input_ids = tokenizer.encode(prompt, add_eos=False)\n",
    "input_ids = torch.tensor(input_ids).unsqueeze(0).to('mps')\n",
    "input_ids"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[15191,   572,   279,  1156,  4767,   315,   279,  3639,  4180,   315]],\n",
       "       device='mps:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c298be7a40fa7d46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T01:09:24.422394Z",
     "start_time": "2025-01-30T01:09:22.651272Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 151936])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(input_ids).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1525f0e1a11c4f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T01:09:48.694219Z",
     "start_time": "2025-01-30T01:09:46.972602Z"
    }
   },
   "outputs": [],
   "source": [
    "outputs = model(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd9176913f128775",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T01:11:46.351828Z",
     "start_time": "2025-01-30T01:11:45.955266Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-105.0000, -139.2857,    1.0547,  ...,  -53.5714,    9.2411,\n",
       "            3.3482]], device='mps:0')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_token_logits = outputs[:, -1, :].float()\n",
    "next_token_logits = next_token_logits / params.temperature\n",
    "next_token_logits"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c5356e9315041ff8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T03:09:08.509183Z",
     "start_time": "2025-01-30T03:09:08.504646Z"
    }
   },
   "cell_type": "code",
   "source": "input_ids.shape",
   "id": "a7c4d987ddace3db",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-30T03:08:35.210089Z",
     "start_time": "2025-01-30T03:08:34.503471Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for key in model.state_dict().keys():\n",
    "    print(f\"{key:<50} | {model.state_dict()[key].dtype} | {model.state_dict()[key].shape}\")"
   ],
   "id": "fae2f4385ff5d2e1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tok_embeddings.weight                              | torch.bfloat16 | torch.Size([151936, 2048])\n",
      "layers.0.attn.q_proj.weight                        | torch.bfloat16 | torch.Size([2048, 2048])\n",
      "layers.0.attn.q_proj.bias                          | torch.bfloat16 | torch.Size([2048])\n",
      "layers.0.attn.k_proj.weight                        | torch.bfloat16 | torch.Size([256, 2048])\n",
      "layers.0.attn.k_proj.bias                          | torch.bfloat16 | torch.Size([256])\n",
      "layers.0.attn.v_proj.weight                        | torch.bfloat16 | torch.Size([256, 2048])\n",
      "layers.0.attn.v_proj.bias                          | torch.bfloat16 | torch.Size([256])\n",
      "layers.0.attn.output_proj.weight                   | torch.bfloat16 | torch.Size([2048, 2048])\n",
      "layers.0.mlp.w1.weight                             | torch.bfloat16 | torch.Size([11008, 2048])\n",
      "layers.0.mlp.w2.weight                             | torch.bfloat16 | torch.Size([2048, 11008])\n",
      "layers.0.mlp.w3.weight                             | torch.bfloat16 | torch.Size([11008, 2048])\n",
      "layers.0.sa_norm.scale                             | torch.bfloat16 | torch.Size([2048])\n",
      "layers.0.mlp_norm.scale                            | torch.bfloat16 | torch.Size([2048])\n",
      "layers.1.attn.q_proj.weight                        | torch.bfloat16 | torch.Size([2048, 2048])\n",
      "layers.1.attn.q_proj.bias                          | torch.bfloat16 | torch.Size([2048])\n",
      "layers.1.attn.k_proj.weight                        | torch.bfloat16 | torch.Size([256, 2048])\n",
      "layers.1.attn.k_proj.bias                          | torch.bfloat16 | torch.Size([256])\n",
      "layers.1.attn.v_proj.weight                        | torch.bfloat16 | torch.Size([256, 2048])\n",
      "layers.1.attn.v_proj.bias                          | torch.bfloat16 | torch.Size([256])\n",
      "layers.1.attn.output_proj.weight                   | torch.bfloat16 | torch.Size([2048, 2048])\n",
      "layers.1.mlp.w1.weight                             | torch.bfloat16 | torch.Size([11008, 2048])\n",
      "layers.1.mlp.w2.weight                             | torch.bfloat16 | torch.Size([2048, 11008])\n",
      "layers.1.mlp.w3.weight                             | torch.bfloat16 | torch.Size([11008, 2048])\n",
      "layers.1.sa_norm.scale                             | torch.bfloat16 | torch.Size([2048])\n",
      "layers.1.mlp_norm.scale                            | torch.bfloat16 | torch.Size([2048])\n",
      "layers.2.attn.q_proj.weight                        | torch.bfloat16 | torch.Size([2048, 2048])\n",
      "layers.2.attn.q_proj.bias                          | torch.bfloat16 | torch.Size([2048])\n",
      "layers.2.attn.k_proj.weight                        | torch.bfloat16 | torch.Size([256, 2048])\n",
      "layers.2.attn.k_proj.bias                          | torch.bfloat16 | torch.Size([256])\n",
      "layers.2.attn.v_proj.weight                        | torch.bfloat16 | torch.Size([256, 2048])\n",
      "layers.2.attn.v_proj.bias                          | torch.bfloat16 | torch.Size([256])\n",
      "layers.2.attn.output_proj.weight                   | torch.bfloat16 | torch.Size([2048, 2048])\n",
      "layers.2.mlp.w1.weight                             | torch.bfloat16 | torch.Size([11008, 2048])\n",
      "layers.2.mlp.w2.weight                             | torch.bfloat16 | torch.Size([2048, 11008])\n",
      "layers.2.mlp.w3.weight                             | torch.bfloat16 | torch.Size([11008, 2048])\n",
      "layers.2.sa_norm.scale                             | torch.bfloat16 | torch.Size([2048])\n",
      "layers.2.mlp_norm.scale                            | torch.bfloat16 | torch.Size([2048])\n",
      "layers.3.attn.q_proj.weight                        | torch.bfloat16 | torch.Size([2048, 2048])\n",
      "layers.3.attn.q_proj.bias                          | torch.bfloat16 | torch.Size([2048])\n",
      "layers.3.attn.k_proj.weight                        | torch.bfloat16 | torch.Size([256, 2048])\n",
      "layers.3.attn.k_proj.bias                          | torch.bfloat16 | torch.Size([256])\n",
      "layers.3.attn.v_proj.weight                        | torch.bfloat16 | torch.Size([256, 2048])\n",
      "layers.3.attn.v_proj.bias                          | torch.bfloat16 | torch.Size([256])\n",
      "layers.3.attn.output_proj.weight                   | torch.bfloat16 | torch.Size([2048, 2048])\n",
      "layers.3.mlp.w1.weight                             | torch.bfloat16 | torch.Size([11008, 2048])\n",
      "layers.3.mlp.w2.weight                             | torch.bfloat16 | torch.Size([2048, 11008])\n",
      "layers.3.mlp.w3.weight                             | torch.bfloat16 | torch.Size([11008, 2048])\n",
      "layers.3.sa_norm.scale                             | torch.bfloat16 | torch.Size([2048])\n",
      "layers.3.mlp_norm.scale                            | torch.bfloat16 | torch.Size([2048])\n",
      "layers.4.attn.q_proj.weight                        | torch.bfloat16 | torch.Size([2048, 2048])\n",
      "layers.4.attn.q_proj.bias                          | torch.bfloat16 | torch.Size([2048])\n",
      "layers.4.attn.k_proj.weight                        | torch.bfloat16 | torch.Size([256, 2048])\n",
      "layers.4.attn.k_proj.bias                          | torch.bfloat16 | torch.Size([256])\n",
      "layers.4.attn.v_proj.weight                        | torch.bfloat16 | torch.Size([256, 2048])\n",
      "layers.4.attn.v_proj.bias                          | torch.bfloat16 | torch.Size([256])\n",
      "layers.4.attn.output_proj.weight                   | torch.bfloat16 | torch.Size([2048, 2048])\n",
      "layers.4.mlp.w1.weight                             | torch.bfloat16 | torch.Size([11008, 2048])\n",
      "layers.4.mlp.w2.weight                             | torch.bfloat16 | torch.Size([2048, 11008])\n",
      "layers.4.mlp.w3.weight                             | torch.bfloat16 | torch.Size([11008, 2048])\n",
      "layers.4.sa_norm.scale                             | torch.bfloat16 | torch.Size([2048])\n",
      "layers.4.mlp_norm.scale                            | torch.bfloat16 | torch.Size([2048])\n",
      "layers.5.attn.q_proj.weight                        | torch.bfloat16 | torch.Size([2048, 2048])\n",
      "layers.5.attn.q_proj.bias                          | torch.bfloat16 | torch.Size([2048])\n",
      "layers.5.attn.k_proj.weight                        | torch.bfloat16 | torch.Size([256, 2048])\n",
      "layers.5.attn.k_proj.bias                          | torch.bfloat16 | torch.Size([256])\n",
      "layers.5.attn.v_proj.weight                        | torch.bfloat16 | torch.Size([256, 2048])\n",
      "layers.5.attn.v_proj.bias                          | torch.bfloat16 | torch.Size([256])\n",
      "layers.5.attn.output_proj.weight                   | torch.bfloat16 | torch.Size([2048, 2048])\n",
      "layers.5.mlp.w1.weight                             | torch.bfloat16 | torch.Size([11008, 2048])\n",
      "layers.5.mlp.w2.weight                             | torch.bfloat16 | torch.Size([2048, 11008])\n",
      "layers.5.mlp.w3.weight                             | torch.bfloat16 | torch.Size([11008, 2048])\n",
      "layers.5.sa_norm.scale                             | torch.bfloat16 | torch.Size([2048])\n",
      "layers.5.mlp_norm.scale                            | torch.bfloat16 | torch.Size([2048])\n",
      "layers.6.attn.q_proj.weight                        | torch.bfloat16 | torch.Size([2048, 2048])\n",
      "layers.6.attn.q_proj.bias                          | torch.bfloat16 | torch.Size([2048])\n",
      "layers.6.attn.k_proj.weight                        | torch.bfloat16 | torch.Size([256, 2048])\n",
      "layers.6.attn.k_proj.bias                          | torch.bfloat16 | torch.Size([256])\n",
      "layers.6.attn.v_proj.weight                        | torch.bfloat16 | torch.Size([256, 2048])\n",
      "layers.6.attn.v_proj.bias                          | torch.bfloat16 | torch.Size([256])\n",
      "layers.6.attn.output_proj.weight                   | torch.bfloat16 | torch.Size([2048, 2048])\n",
      "layers.6.mlp.w1.weight                             | torch.bfloat16 | torch.Size([11008, 2048])\n",
      "layers.6.mlp.w2.weight                             | torch.bfloat16 | torch.Size([2048, 11008])\n",
      "layers.6.mlp.w3.weight                             | torch.bfloat16 | torch.Size([11008, 2048])\n",
      "layers.6.sa_norm.scale                             | torch.bfloat16 | torch.Size([2048])\n",
      "layers.6.mlp_norm.scale                            | torch.bfloat16 | torch.Size([2048])\n",
      "layers.7.attn.q_proj.weight                        | torch.bfloat16 | torch.Size([2048, 2048])\n",
      "layers.7.attn.q_proj.bias                          | torch.bfloat16 | torch.Size([2048])\n",
      "layers.7.attn.k_proj.weight                        | torch.bfloat16 | torch.Size([256, 2048])\n",
      "layers.7.attn.k_proj.bias                          | torch.bfloat16 | torch.Size([256])\n",
      "layers.7.attn.v_proj.weight                        | torch.bfloat16 | torch.Size([256, 2048])\n",
      "layers.7.attn.v_proj.bias                          | torch.bfloat16 | torch.Size([256])\n",
      "layers.7.attn.output_proj.weight                   | torch.bfloat16 | torch.Size([2048, 2048])\n",
      "layers.7.mlp.w1.weight                             | torch.bfloat16 | torch.Size([11008, 2048])\n",
      "layers.7.mlp.w2.weight                             | torch.bfloat16 | torch.Size([2048, 11008])\n",
      "layers.7.mlp.w3.weight                             | torch.bfloat16 | torch.Size([11008, 2048])\n",
      "layers.7.sa_norm.scale                             | torch.bfloat16 | torch.Size([2048])\n",
      "layers.7.mlp_norm.scale                            | torch.bfloat16 | torch.Size([2048])\n",
      "layers.8.attn.q_proj.weight                        | torch.bfloat16 | torch.Size([2048, 2048])\n",
      "layers.8.attn.q_proj.bias                          | torch.bfloat16 | torch.Size([2048])\n",
      "layers.8.attn.k_proj.weight                        | torch.bfloat16 | torch.Size([256, 2048])\n",
      "layers.8.attn.k_proj.bias                          | torch.bfloat16 | torch.Size([256])\n",
      "layers.8.attn.v_proj.weight                        | torch.bfloat16 | torch.Size([256, 2048])\n",
      "layers.8.attn.v_proj.bias                          | torch.bfloat16 | torch.Size([256])\n",
      "layers.8.attn.output_proj.weight                   | torch.bfloat16 | torch.Size([2048, 2048])\n",
      "layers.8.mlp.w1.weight                             | torch.bfloat16 | torch.Size([11008, 2048])\n",
      "layers.8.mlp.w2.weight                             | torch.bfloat16 | torch.Size([2048, 11008])\n",
      "layers.8.mlp.w3.weight                             | torch.bfloat16 | torch.Size([11008, 2048])\n",
      "layers.8.sa_norm.scale                             | torch.bfloat16 | torch.Size([2048])\n",
      "layers.8.mlp_norm.scale                            | torch.bfloat16 | torch.Size([2048])\n",
      "layers.9.attn.q_proj.weight                        | torch.bfloat16 | torch.Size([2048, 2048])\n",
      "layers.9.attn.q_proj.bias                          | torch.bfloat16 | torch.Size([2048])\n",
      "layers.9.attn.k_proj.weight                        | torch.bfloat16 | torch.Size([256, 2048])\n",
      "layers.9.attn.k_proj.bias                          | torch.bfloat16 | torch.Size([256])\n",
      "layers.9.attn.v_proj.weight                        | torch.bfloat16 | torch.Size([256, 2048])\n",
      "layers.9.attn.v_proj.bias                          | torch.bfloat16 | torch.Size([256])\n",
      "layers.9.attn.output_proj.weight                   | torch.bfloat16 | torch.Size([2048, 2048])\n",
      "layers.9.mlp.w1.weight                             | torch.bfloat16 | torch.Size([11008, 2048])\n",
      "layers.9.mlp.w2.weight                             | torch.bfloat16 | torch.Size([2048, 11008])\n",
      "layers.9.mlp.w3.weight                             | torch.bfloat16 | torch.Size([11008, 2048])\n",
      "layers.9.sa_norm.scale                             | torch.bfloat16 | torch.Size([2048])\n",
      "layers.9.mlp_norm.scale                            | torch.bfloat16 | torch.Size([2048])\n",
      "layers.10.attn.q_proj.weight                       | torch.bfloat16 | torch.Size([2048, 2048])\n",
      "layers.10.attn.q_proj.bias                         | torch.bfloat16 | torch.Size([2048])\n",
      "layers.10.attn.k_proj.weight                       | torch.bfloat16 | torch.Size([256, 2048])\n",
      "layers.10.attn.k_proj.bias                         | torch.bfloat16 | torch.Size([256])\n",
      "layers.10.attn.v_proj.weight                       | torch.bfloat16 | torch.Size([256, 2048])\n",
      "layers.10.attn.v_proj.bias                         | torch.bfloat16 | torch.Size([256])\n",
      "layers.10.attn.output_proj.weight                  | torch.bfloat16 | torch.Size([2048, 2048])\n",
      "layers.10.mlp.w1.weight                            | torch.bfloat16 | torch.Size([11008, 2048])\n",
      "layers.10.mlp.w2.weight                            | torch.bfloat16 | torch.Size([2048, 11008])\n",
      "layers.10.mlp.w3.weight                            | torch.bfloat16 | torch.Size([11008, 2048])\n",
      "layers.10.sa_norm.scale                            | torch.bfloat16 | torch.Size([2048])\n",
      "layers.10.mlp_norm.scale                           | torch.bfloat16 | torch.Size([2048])\n",
      "layers.11.attn.q_proj.weight                       | torch.bfloat16 | torch.Size([2048, 2048])\n",
      "layers.11.attn.q_proj.bias                         | torch.bfloat16 | torch.Size([2048])\n",
      "layers.11.attn.k_proj.weight                       | torch.bfloat16 | torch.Size([256, 2048])\n",
      "layers.11.attn.k_proj.bias                         | torch.bfloat16 | torch.Size([256])\n",
      "layers.11.attn.v_proj.weight                       | torch.bfloat16 | torch.Size([256, 2048])\n",
      "layers.11.attn.v_proj.bias                         | torch.bfloat16 | torch.Size([256])\n",
      "layers.11.attn.output_proj.weight                  | torch.bfloat16 | torch.Size([2048, 2048])\n",
      "layers.11.mlp.w1.weight                            | torch.bfloat16 | torch.Size([11008, 2048])\n",
      "layers.11.mlp.w2.weight                            | torch.bfloat16 | torch.Size([2048, 11008])\n",
      "layers.11.mlp.w3.weight                            | torch.bfloat16 | torch.Size([11008, 2048])\n",
      "layers.11.sa_norm.scale                            | torch.bfloat16 | torch.Size([2048])\n",
      "layers.11.mlp_norm.scale                           | torch.bfloat16 | torch.Size([2048])\n",
      "layers.12.attn.q_proj.weight                       | torch.bfloat16 | torch.Size([2048, 2048])\n",
      "layers.12.attn.q_proj.bias                         | torch.bfloat16 | torch.Size([2048])\n",
      "layers.12.attn.k_proj.weight                       | torch.bfloat16 | torch.Size([256, 2048])\n",
      "layers.12.attn.k_proj.bias                         | torch.bfloat16 | torch.Size([256])\n",
      "layers.12.attn.v_proj.weight                       | torch.bfloat16 | torch.Size([256, 2048])\n",
      "layers.12.attn.v_proj.bias                         | torch.bfloat16 | torch.Size([256])\n",
      "layers.12.attn.output_proj.weight                  | torch.bfloat16 | torch.Size([2048, 2048])\n",
      "layers.12.mlp.w1.weight                            | torch.bfloat16 | torch.Size([11008, 2048])\n",
      "layers.12.mlp.w2.weight                            | torch.bfloat16 | torch.Size([2048, 11008])\n",
      "layers.12.mlp.w3.weight                            | torch.bfloat16 | torch.Size([11008, 2048])\n",
      "layers.12.sa_norm.scale                            | torch.bfloat16 | torch.Size([2048])\n",
      "layers.12.mlp_norm.scale                           | torch.bfloat16 | torch.Size([2048])\n",
      "layers.13.attn.q_proj.weight                       | torch.bfloat16 | torch.Size([2048, 2048])\n",
      "layers.13.attn.q_proj.bias                         | torch.bfloat16 | torch.Size([2048])\n",
      "layers.13.attn.k_proj.weight                       | torch.bfloat16 | torch.Size([256, 2048])\n",
      "layers.13.attn.k_proj.bias                         | torch.bfloat16 | torch.Size([256])\n",
      "layers.13.attn.v_proj.weight                       | torch.bfloat16 | torch.Size([256, 2048])\n",
      "layers.13.attn.v_proj.bias                         | torch.bfloat16 | torch.Size([256])\n",
      "layers.13.attn.output_proj.weight                  | torch.bfloat16 | torch.Size([2048, 2048])\n",
      "layers.13.mlp.w1.weight                            | torch.bfloat16 | torch.Size([11008, 2048])\n",
      "layers.13.mlp.w2.weight                            | torch.bfloat16 | torch.Size([2048, 11008])\n",
      "layers.13.mlp.w3.weight                            | torch.bfloat16 | torch.Size([11008, 2048])\n",
      "layers.13.sa_norm.scale                            | torch.bfloat16 | torch.Size([2048])\n",
      "layers.13.mlp_norm.scale                           | torch.bfloat16 | torch.Size([2048])\n",
      "layers.14.attn.q_proj.weight                       | torch.bfloat16 | torch.Size([2048, 2048])\n",
      "layers.14.attn.q_proj.bias                         | torch.bfloat16 | torch.Size([2048])\n",
      "layers.14.attn.k_proj.weight                       | torch.bfloat16 | torch.Size([256, 2048])\n",
      "layers.14.attn.k_proj.bias                         | torch.bfloat16 | torch.Size([256])\n",
      "layers.14.attn.v_proj.weight                       | torch.bfloat16 | torch.Size([256, 2048])\n",
      "layers.14.attn.v_proj.bias                         | torch.bfloat16 | torch.Size([256])\n",
      "layers.14.attn.output_proj.weight                  | torch.bfloat16 | torch.Size([2048, 2048])\n",
      "layers.14.mlp.w1.weight                            | torch.bfloat16 | torch.Size([11008, 2048])\n",
      "layers.14.mlp.w2.weight                            | torch.bfloat16 | torch.Size([2048, 11008])\n",
      "layers.14.mlp.w3.weight                            | torch.bfloat16 | torch.Size([11008, 2048])\n",
      "layers.14.sa_norm.scale                            | torch.bfloat16 | torch.Size([2048])\n",
      "layers.14.mlp_norm.scale                           | torch.bfloat16 | torch.Size([2048])\n",
      "layers.15.attn.q_proj.weight                       | torch.bfloat16 | torch.Size([2048, 2048])\n",
      "layers.15.attn.q_proj.bias                         | torch.bfloat16 | torch.Size([2048])\n",
      "layers.15.attn.k_proj.weight                       | torch.bfloat16 | torch.Size([256, 2048])\n",
      "layers.15.attn.k_proj.bias                         | torch.bfloat16 | torch.Size([256])\n",
      "layers.15.attn.v_proj.weight                       | torch.bfloat16 | torch.Size([256, 2048])\n",
      "layers.15.attn.v_proj.bias                         | torch.bfloat16 | torch.Size([256])\n",
      "layers.15.attn.output_proj.weight                  | torch.bfloat16 | torch.Size([2048, 2048])\n",
      "layers.15.mlp.w1.weight                            | torch.bfloat16 | torch.Size([11008, 2048])\n",
      "layers.15.mlp.w2.weight                            | torch.bfloat16 | torch.Size([2048, 11008])\n",
      "layers.15.mlp.w3.weight                            | torch.bfloat16 | torch.Size([11008, 2048])\n",
      "layers.15.sa_norm.scale                            | torch.bfloat16 | torch.Size([2048])\n",
      "layers.15.mlp_norm.scale                           | torch.bfloat16 | torch.Size([2048])\n",
      "layers.16.attn.q_proj.weight                       | torch.bfloat16 | torch.Size([2048, 2048])\n",
      "layers.16.attn.q_proj.bias                         | torch.bfloat16 | torch.Size([2048])\n",
      "layers.16.attn.k_proj.weight                       | torch.bfloat16 | torch.Size([256, 2048])\n",
      "layers.16.attn.k_proj.bias                         | torch.bfloat16 | torch.Size([256])\n",
      "layers.16.attn.v_proj.weight                       | torch.bfloat16 | torch.Size([256, 2048])\n",
      "layers.16.attn.v_proj.bias                         | torch.bfloat16 | torch.Size([256])\n",
      "layers.16.attn.output_proj.weight                  | torch.bfloat16 | torch.Size([2048, 2048])\n",
      "layers.16.mlp.w1.weight                            | torch.bfloat16 | torch.Size([11008, 2048])\n",
      "layers.16.mlp.w2.weight                            | torch.bfloat16 | torch.Size([2048, 11008])\n",
      "layers.16.mlp.w3.weight                            | torch.bfloat16 | torch.Size([11008, 2048])\n",
      "layers.16.sa_norm.scale                            | torch.bfloat16 | torch.Size([2048])\n",
      "layers.16.mlp_norm.scale                           | torch.bfloat16 | torch.Size([2048])\n",
      "layers.17.attn.q_proj.weight                       | torch.bfloat16 | torch.Size([2048, 2048])\n",
      "layers.17.attn.q_proj.bias                         | torch.bfloat16 | torch.Size([2048])\n",
      "layers.17.attn.k_proj.weight                       | torch.bfloat16 | torch.Size([256, 2048])\n",
      "layers.17.attn.k_proj.bias                         | torch.bfloat16 | torch.Size([256])\n",
      "layers.17.attn.v_proj.weight                       | torch.bfloat16 | torch.Size([256, 2048])\n",
      "layers.17.attn.v_proj.bias                         | torch.bfloat16 | torch.Size([256])\n",
      "layers.17.attn.output_proj.weight                  | torch.bfloat16 | torch.Size([2048, 2048])\n",
      "layers.17.mlp.w1.weight                            | torch.bfloat16 | torch.Size([11008, 2048])\n",
      "layers.17.mlp.w2.weight                            | torch.bfloat16 | torch.Size([2048, 11008])\n",
      "layers.17.mlp.w3.weight                            | torch.bfloat16 | torch.Size([11008, 2048])\n",
      "layers.17.sa_norm.scale                            | torch.bfloat16 | torch.Size([2048])\n",
      "layers.17.mlp_norm.scale                           | torch.bfloat16 | torch.Size([2048])\n",
      "layers.18.attn.q_proj.weight                       | torch.bfloat16 | torch.Size([2048, 2048])\n",
      "layers.18.attn.q_proj.bias                         | torch.bfloat16 | torch.Size([2048])\n",
      "layers.18.attn.k_proj.weight                       | torch.bfloat16 | torch.Size([256, 2048])\n",
      "layers.18.attn.k_proj.bias                         | torch.bfloat16 | torch.Size([256])\n",
      "layers.18.attn.v_proj.weight                       | torch.bfloat16 | torch.Size([256, 2048])\n",
      "layers.18.attn.v_proj.bias                         | torch.bfloat16 | torch.Size([256])\n",
      "layers.18.attn.output_proj.weight                  | torch.bfloat16 | torch.Size([2048, 2048])\n",
      "layers.18.mlp.w1.weight                            | torch.bfloat16 | torch.Size([11008, 2048])\n",
      "layers.18.mlp.w2.weight                            | torch.bfloat16 | torch.Size([2048, 11008])\n",
      "layers.18.mlp.w3.weight                            | torch.bfloat16 | torch.Size([11008, 2048])\n",
      "layers.18.sa_norm.scale                            | torch.bfloat16 | torch.Size([2048])\n",
      "layers.18.mlp_norm.scale                           | torch.bfloat16 | torch.Size([2048])\n",
      "layers.19.attn.q_proj.weight                       | torch.bfloat16 | torch.Size([2048, 2048])\n",
      "layers.19.attn.q_proj.bias                         | torch.bfloat16 | torch.Size([2048])\n",
      "layers.19.attn.k_proj.weight                       | torch.bfloat16 | torch.Size([256, 2048])\n",
      "layers.19.attn.k_proj.bias                         | torch.bfloat16 | torch.Size([256])\n",
      "layers.19.attn.v_proj.weight                       | torch.bfloat16 | torch.Size([256, 2048])\n",
      "layers.19.attn.v_proj.bias                         | torch.bfloat16 | torch.Size([256])\n",
      "layers.19.attn.output_proj.weight                  | torch.bfloat16 | torch.Size([2048, 2048])\n",
      "layers.19.mlp.w1.weight                            | torch.bfloat16 | torch.Size([11008, 2048])\n",
      "layers.19.mlp.w2.weight                            | torch.bfloat16 | torch.Size([2048, 11008])\n",
      "layers.19.mlp.w3.weight                            | torch.bfloat16 | torch.Size([11008, 2048])\n",
      "layers.19.sa_norm.scale                            | torch.bfloat16 | torch.Size([2048])\n",
      "layers.19.mlp_norm.scale                           | torch.bfloat16 | torch.Size([2048])\n",
      "layers.20.attn.q_proj.weight                       | torch.bfloat16 | torch.Size([2048, 2048])\n",
      "layers.20.attn.q_proj.bias                         | torch.bfloat16 | torch.Size([2048])\n",
      "layers.20.attn.k_proj.weight                       | torch.bfloat16 | torch.Size([256, 2048])\n",
      "layers.20.attn.k_proj.bias                         | torch.bfloat16 | torch.Size([256])\n",
      "layers.20.attn.v_proj.weight                       | torch.bfloat16 | torch.Size([256, 2048])\n",
      "layers.20.attn.v_proj.bias                         | torch.bfloat16 | torch.Size([256])\n",
      "layers.20.attn.output_proj.weight                  | torch.bfloat16 | torch.Size([2048, 2048])\n",
      "layers.20.mlp.w1.weight                            | torch.bfloat16 | torch.Size([11008, 2048])\n",
      "layers.20.mlp.w2.weight                            | torch.bfloat16 | torch.Size([2048, 11008])\n",
      "layers.20.mlp.w3.weight                            | torch.bfloat16 | torch.Size([11008, 2048])\n",
      "layers.20.sa_norm.scale                            | torch.bfloat16 | torch.Size([2048])\n",
      "layers.20.mlp_norm.scale                           | torch.bfloat16 | torch.Size([2048])\n",
      "layers.21.attn.q_proj.weight                       | torch.bfloat16 | torch.Size([2048, 2048])\n",
      "layers.21.attn.q_proj.bias                         | torch.bfloat16 | torch.Size([2048])\n",
      "layers.21.attn.k_proj.weight                       | torch.bfloat16 | torch.Size([256, 2048])\n",
      "layers.21.attn.k_proj.bias                         | torch.bfloat16 | torch.Size([256])\n",
      "layers.21.attn.v_proj.weight                       | torch.bfloat16 | torch.Size([256, 2048])\n",
      "layers.21.attn.v_proj.bias                         | torch.bfloat16 | torch.Size([256])\n",
      "layers.21.attn.output_proj.weight                  | torch.bfloat16 | torch.Size([2048, 2048])\n",
      "layers.21.mlp.w1.weight                            | torch.bfloat16 | torch.Size([11008, 2048])\n",
      "layers.21.mlp.w2.weight                            | torch.bfloat16 | torch.Size([2048, 11008])\n",
      "layers.21.mlp.w3.weight                            | torch.bfloat16 | torch.Size([11008, 2048])\n",
      "layers.21.sa_norm.scale                            | torch.bfloat16 | torch.Size([2048])\n",
      "layers.21.mlp_norm.scale                           | torch.bfloat16 | torch.Size([2048])\n",
      "layers.22.attn.q_proj.weight                       | torch.bfloat16 | torch.Size([2048, 2048])\n",
      "layers.22.attn.q_proj.bias                         | torch.bfloat16 | torch.Size([2048])\n",
      "layers.22.attn.k_proj.weight                       | torch.bfloat16 | torch.Size([256, 2048])\n",
      "layers.22.attn.k_proj.bias                         | torch.bfloat16 | torch.Size([256])\n",
      "layers.22.attn.v_proj.weight                       | torch.bfloat16 | torch.Size([256, 2048])\n",
      "layers.22.attn.v_proj.bias                         | torch.bfloat16 | torch.Size([256])\n",
      "layers.22.attn.output_proj.weight                  | torch.bfloat16 | torch.Size([2048, 2048])\n",
      "layers.22.mlp.w1.weight                            | torch.bfloat16 | torch.Size([11008, 2048])\n",
      "layers.22.mlp.w2.weight                            | torch.bfloat16 | torch.Size([2048, 11008])\n",
      "layers.22.mlp.w3.weight                            | torch.bfloat16 | torch.Size([11008, 2048])\n",
      "layers.22.sa_norm.scale                            | torch.bfloat16 | torch.Size([2048])\n",
      "layers.22.mlp_norm.scale                           | torch.bfloat16 | torch.Size([2048])\n",
      "layers.23.attn.q_proj.weight                       | torch.bfloat16 | torch.Size([2048, 2048])\n",
      "layers.23.attn.q_proj.bias                         | torch.bfloat16 | torch.Size([2048])\n",
      "layers.23.attn.k_proj.weight                       | torch.bfloat16 | torch.Size([256, 2048])\n",
      "layers.23.attn.k_proj.bias                         | torch.bfloat16 | torch.Size([256])\n",
      "layers.23.attn.v_proj.weight                       | torch.bfloat16 | torch.Size([256, 2048])\n",
      "layers.23.attn.v_proj.bias                         | torch.bfloat16 | torch.Size([256])\n",
      "layers.23.attn.output_proj.weight                  | torch.bfloat16 | torch.Size([2048, 2048])\n",
      "layers.23.mlp.w1.weight                            | torch.bfloat16 | torch.Size([11008, 2048])\n",
      "layers.23.mlp.w2.weight                            | torch.bfloat16 | torch.Size([2048, 11008])\n",
      "layers.23.mlp.w3.weight                            | torch.bfloat16 | torch.Size([11008, 2048])\n",
      "layers.23.sa_norm.scale                            | torch.bfloat16 | torch.Size([2048])\n",
      "layers.23.mlp_norm.scale                           | torch.bfloat16 | torch.Size([2048])\n",
      "layers.24.attn.q_proj.weight                       | torch.bfloat16 | torch.Size([2048, 2048])\n",
      "layers.24.attn.q_proj.bias                         | torch.bfloat16 | torch.Size([2048])\n",
      "layers.24.attn.k_proj.weight                       | torch.bfloat16 | torch.Size([256, 2048])\n",
      "layers.24.attn.k_proj.bias                         | torch.bfloat16 | torch.Size([256])\n",
      "layers.24.attn.v_proj.weight                       | torch.bfloat16 | torch.Size([256, 2048])\n",
      "layers.24.attn.v_proj.bias                         | torch.bfloat16 | torch.Size([256])\n",
      "layers.24.attn.output_proj.weight                  | torch.bfloat16 | torch.Size([2048, 2048])\n",
      "layers.24.mlp.w1.weight                            | torch.bfloat16 | torch.Size([11008, 2048])\n",
      "layers.24.mlp.w2.weight                            | torch.bfloat16 | torch.Size([2048, 11008])\n",
      "layers.24.mlp.w3.weight                            | torch.bfloat16 | torch.Size([11008, 2048])\n",
      "layers.24.sa_norm.scale                            | torch.bfloat16 | torch.Size([2048])\n",
      "layers.24.mlp_norm.scale                           | torch.bfloat16 | torch.Size([2048])\n",
      "layers.25.attn.q_proj.weight                       | torch.bfloat16 | torch.Size([2048, 2048])\n",
      "layers.25.attn.q_proj.bias                         | torch.bfloat16 | torch.Size([2048])\n",
      "layers.25.attn.k_proj.weight                       | torch.bfloat16 | torch.Size([256, 2048])\n",
      "layers.25.attn.k_proj.bias                         | torch.bfloat16 | torch.Size([256])\n",
      "layers.25.attn.v_proj.weight                       | torch.bfloat16 | torch.Size([256, 2048])\n",
      "layers.25.attn.v_proj.bias                         | torch.bfloat16 | torch.Size([256])\n",
      "layers.25.attn.output_proj.weight                  | torch.bfloat16 | torch.Size([2048, 2048])\n",
      "layers.25.mlp.w1.weight                            | torch.bfloat16 | torch.Size([11008, 2048])\n",
      "layers.25.mlp.w2.weight                            | torch.bfloat16 | torch.Size([2048, 11008])\n",
      "layers.25.mlp.w3.weight                            | torch.bfloat16 | torch.Size([11008, 2048])\n",
      "layers.25.sa_norm.scale                            | torch.bfloat16 | torch.Size([2048])\n",
      "layers.25.mlp_norm.scale                           | torch.bfloat16 | torch.Size([2048])\n",
      "layers.26.attn.q_proj.weight                       | torch.bfloat16 | torch.Size([2048, 2048])\n",
      "layers.26.attn.q_proj.bias                         | torch.bfloat16 | torch.Size([2048])\n",
      "layers.26.attn.k_proj.weight                       | torch.bfloat16 | torch.Size([256, 2048])\n",
      "layers.26.attn.k_proj.bias                         | torch.bfloat16 | torch.Size([256])\n",
      "layers.26.attn.v_proj.weight                       | torch.bfloat16 | torch.Size([256, 2048])\n",
      "layers.26.attn.v_proj.bias                         | torch.bfloat16 | torch.Size([256])\n",
      "layers.26.attn.output_proj.weight                  | torch.bfloat16 | torch.Size([2048, 2048])\n",
      "layers.26.mlp.w1.weight                            | torch.bfloat16 | torch.Size([11008, 2048])\n",
      "layers.26.mlp.w2.weight                            | torch.bfloat16 | torch.Size([2048, 11008])\n",
      "layers.26.mlp.w3.weight                            | torch.bfloat16 | torch.Size([11008, 2048])\n",
      "layers.26.sa_norm.scale                            | torch.bfloat16 | torch.Size([2048])\n",
      "layers.26.mlp_norm.scale                           | torch.bfloat16 | torch.Size([2048])\n",
      "layers.27.attn.q_proj.weight                       | torch.bfloat16 | torch.Size([2048, 2048])\n",
      "layers.27.attn.q_proj.bias                         | torch.bfloat16 | torch.Size([2048])\n",
      "layers.27.attn.k_proj.weight                       | torch.bfloat16 | torch.Size([256, 2048])\n",
      "layers.27.attn.k_proj.bias                         | torch.bfloat16 | torch.Size([256])\n",
      "layers.27.attn.v_proj.weight                       | torch.bfloat16 | torch.Size([256, 2048])\n",
      "layers.27.attn.v_proj.bias                         | torch.bfloat16 | torch.Size([256])\n",
      "layers.27.attn.output_proj.weight                  | torch.bfloat16 | torch.Size([2048, 2048])\n",
      "layers.27.mlp.w1.weight                            | torch.bfloat16 | torch.Size([11008, 2048])\n",
      "layers.27.mlp.w2.weight                            | torch.bfloat16 | torch.Size([2048, 11008])\n",
      "layers.27.mlp.w3.weight                            | torch.bfloat16 | torch.Size([11008, 2048])\n",
      "layers.27.sa_norm.scale                            | torch.bfloat16 | torch.Size([2048])\n",
      "layers.27.mlp_norm.scale                           | torch.bfloat16 | torch.Size([2048])\n",
      "layers.28.attn.q_proj.weight                       | torch.bfloat16 | torch.Size([2048, 2048])\n",
      "layers.28.attn.q_proj.bias                         | torch.bfloat16 | torch.Size([2048])\n",
      "layers.28.attn.k_proj.weight                       | torch.bfloat16 | torch.Size([256, 2048])\n",
      "layers.28.attn.k_proj.bias                         | torch.bfloat16 | torch.Size([256])\n",
      "layers.28.attn.v_proj.weight                       | torch.bfloat16 | torch.Size([256, 2048])\n",
      "layers.28.attn.v_proj.bias                         | torch.bfloat16 | torch.Size([256])\n",
      "layers.28.attn.output_proj.weight                  | torch.bfloat16 | torch.Size([2048, 2048])\n",
      "layers.28.mlp.w1.weight                            | torch.bfloat16 | torch.Size([11008, 2048])\n",
      "layers.28.mlp.w2.weight                            | torch.bfloat16 | torch.Size([2048, 11008])\n",
      "layers.28.mlp.w3.weight                            | torch.bfloat16 | torch.Size([11008, 2048])\n",
      "layers.28.sa_norm.scale                            | torch.bfloat16 | torch.Size([2048])\n",
      "layers.28.mlp_norm.scale                           | torch.bfloat16 | torch.Size([2048])\n",
      "layers.29.attn.q_proj.weight                       | torch.bfloat16 | torch.Size([2048, 2048])\n",
      "layers.29.attn.q_proj.bias                         | torch.bfloat16 | torch.Size([2048])\n",
      "layers.29.attn.k_proj.weight                       | torch.bfloat16 | torch.Size([256, 2048])\n",
      "layers.29.attn.k_proj.bias                         | torch.bfloat16 | torch.Size([256])\n",
      "layers.29.attn.v_proj.weight                       | torch.bfloat16 | torch.Size([256, 2048])\n",
      "layers.29.attn.v_proj.bias                         | torch.bfloat16 | torch.Size([256])\n",
      "layers.29.attn.output_proj.weight                  | torch.bfloat16 | torch.Size([2048, 2048])\n",
      "layers.29.mlp.w1.weight                            | torch.bfloat16 | torch.Size([11008, 2048])\n",
      "layers.29.mlp.w2.weight                            | torch.bfloat16 | torch.Size([2048, 11008])\n",
      "layers.29.mlp.w3.weight                            | torch.bfloat16 | torch.Size([11008, 2048])\n",
      "layers.29.sa_norm.scale                            | torch.bfloat16 | torch.Size([2048])\n",
      "layers.29.mlp_norm.scale                           | torch.bfloat16 | torch.Size([2048])\n",
      "layers.30.attn.q_proj.weight                       | torch.bfloat16 | torch.Size([2048, 2048])\n",
      "layers.30.attn.q_proj.bias                         | torch.bfloat16 | torch.Size([2048])\n",
      "layers.30.attn.k_proj.weight                       | torch.bfloat16 | torch.Size([256, 2048])\n",
      "layers.30.attn.k_proj.bias                         | torch.bfloat16 | torch.Size([256])\n",
      "layers.30.attn.v_proj.weight                       | torch.bfloat16 | torch.Size([256, 2048])\n",
      "layers.30.attn.v_proj.bias                         | torch.bfloat16 | torch.Size([256])\n",
      "layers.30.attn.output_proj.weight                  | torch.bfloat16 | torch.Size([2048, 2048])\n",
      "layers.30.mlp.w1.weight                            | torch.bfloat16 | torch.Size([11008, 2048])\n",
      "layers.30.mlp.w2.weight                            | torch.bfloat16 | torch.Size([2048, 11008])\n",
      "layers.30.mlp.w3.weight                            | torch.bfloat16 | torch.Size([11008, 2048])\n",
      "layers.30.sa_norm.scale                            | torch.bfloat16 | torch.Size([2048])\n",
      "layers.30.mlp_norm.scale                           | torch.bfloat16 | torch.Size([2048])\n",
      "layers.31.attn.q_proj.weight                       | torch.bfloat16 | torch.Size([2048, 2048])\n",
      "layers.31.attn.q_proj.bias                         | torch.bfloat16 | torch.Size([2048])\n",
      "layers.31.attn.k_proj.weight                       | torch.bfloat16 | torch.Size([256, 2048])\n",
      "layers.31.attn.k_proj.bias                         | torch.bfloat16 | torch.Size([256])\n",
      "layers.31.attn.v_proj.weight                       | torch.bfloat16 | torch.Size([256, 2048])\n",
      "layers.31.attn.v_proj.bias                         | torch.bfloat16 | torch.Size([256])\n",
      "layers.31.attn.output_proj.weight                  | torch.bfloat16 | torch.Size([2048, 2048])\n",
      "layers.31.mlp.w1.weight                            | torch.bfloat16 | torch.Size([11008, 2048])\n",
      "layers.31.mlp.w2.weight                            | torch.bfloat16 | torch.Size([2048, 11008])\n",
      "layers.31.mlp.w3.weight                            | torch.bfloat16 | torch.Size([11008, 2048])\n",
      "layers.31.sa_norm.scale                            | torch.bfloat16 | torch.Size([2048])\n",
      "layers.31.mlp_norm.scale                           | torch.bfloat16 | torch.Size([2048])\n",
      "layers.32.attn.q_proj.weight                       | torch.bfloat16 | torch.Size([2048, 2048])\n",
      "layers.32.attn.q_proj.bias                         | torch.bfloat16 | torch.Size([2048])\n",
      "layers.32.attn.k_proj.weight                       | torch.bfloat16 | torch.Size([256, 2048])\n",
      "layers.32.attn.k_proj.bias                         | torch.bfloat16 | torch.Size([256])\n",
      "layers.32.attn.v_proj.weight                       | torch.bfloat16 | torch.Size([256, 2048])\n",
      "layers.32.attn.v_proj.bias                         | torch.bfloat16 | torch.Size([256])\n",
      "layers.32.attn.output_proj.weight                  | torch.bfloat16 | torch.Size([2048, 2048])\n",
      "layers.32.mlp.w1.weight                            | torch.bfloat16 | torch.Size([11008, 2048])\n",
      "layers.32.mlp.w2.weight                            | torch.bfloat16 | torch.Size([2048, 11008])\n",
      "layers.32.mlp.w3.weight                            | torch.bfloat16 | torch.Size([11008, 2048])\n",
      "layers.32.sa_norm.scale                            | torch.bfloat16 | torch.Size([2048])\n",
      "layers.32.mlp_norm.scale                           | torch.bfloat16 | torch.Size([2048])\n",
      "layers.33.attn.q_proj.weight                       | torch.bfloat16 | torch.Size([2048, 2048])\n",
      "layers.33.attn.q_proj.bias                         | torch.bfloat16 | torch.Size([2048])\n",
      "layers.33.attn.k_proj.weight                       | torch.bfloat16 | torch.Size([256, 2048])\n",
      "layers.33.attn.k_proj.bias                         | torch.bfloat16 | torch.Size([256])\n",
      "layers.33.attn.v_proj.weight                       | torch.bfloat16 | torch.Size([256, 2048])\n",
      "layers.33.attn.v_proj.bias                         | torch.bfloat16 | torch.Size([256])\n",
      "layers.33.attn.output_proj.weight                  | torch.bfloat16 | torch.Size([2048, 2048])\n",
      "layers.33.mlp.w1.weight                            | torch.bfloat16 | torch.Size([11008, 2048])\n",
      "layers.33.mlp.w2.weight                            | torch.bfloat16 | torch.Size([2048, 11008])\n",
      "layers.33.mlp.w3.weight                            | torch.bfloat16 | torch.Size([11008, 2048])\n",
      "layers.33.sa_norm.scale                            | torch.bfloat16 | torch.Size([2048])\n",
      "layers.33.mlp_norm.scale                           | torch.bfloat16 | torch.Size([2048])\n",
      "layers.34.attn.q_proj.weight                       | torch.bfloat16 | torch.Size([2048, 2048])\n",
      "layers.34.attn.q_proj.bias                         | torch.bfloat16 | torch.Size([2048])\n",
      "layers.34.attn.k_proj.weight                       | torch.bfloat16 | torch.Size([256, 2048])\n",
      "layers.34.attn.k_proj.bias                         | torch.bfloat16 | torch.Size([256])\n",
      "layers.34.attn.v_proj.weight                       | torch.bfloat16 | torch.Size([256, 2048])\n",
      "layers.34.attn.v_proj.bias                         | torch.bfloat16 | torch.Size([256])\n",
      "layers.34.attn.output_proj.weight                  | torch.bfloat16 | torch.Size([2048, 2048])\n",
      "layers.34.mlp.w1.weight                            | torch.bfloat16 | torch.Size([11008, 2048])\n",
      "layers.34.mlp.w2.weight                            | torch.bfloat16 | torch.Size([2048, 11008])\n",
      "layers.34.mlp.w3.weight                            | torch.bfloat16 | torch.Size([11008, 2048])\n",
      "layers.34.sa_norm.scale                            | torch.bfloat16 | torch.Size([2048])\n",
      "layers.34.mlp_norm.scale                           | torch.bfloat16 | torch.Size([2048])\n",
      "layers.35.attn.q_proj.weight                       | torch.bfloat16 | torch.Size([2048, 2048])\n",
      "layers.35.attn.q_proj.bias                         | torch.bfloat16 | torch.Size([2048])\n",
      "layers.35.attn.k_proj.weight                       | torch.bfloat16 | torch.Size([256, 2048])\n",
      "layers.35.attn.k_proj.bias                         | torch.bfloat16 | torch.Size([256])\n",
      "layers.35.attn.v_proj.weight                       | torch.bfloat16 | torch.Size([256, 2048])\n",
      "layers.35.attn.v_proj.bias                         | torch.bfloat16 | torch.Size([256])\n",
      "layers.35.attn.output_proj.weight                  | torch.bfloat16 | torch.Size([2048, 2048])\n",
      "layers.35.mlp.w1.weight                            | torch.bfloat16 | torch.Size([11008, 2048])\n",
      "layers.35.mlp.w2.weight                            | torch.bfloat16 | torch.Size([2048, 11008])\n",
      "layers.35.mlp.w3.weight                            | torch.bfloat16 | torch.Size([11008, 2048])\n",
      "layers.35.sa_norm.scale                            | torch.bfloat16 | torch.Size([2048])\n",
      "layers.35.mlp_norm.scale                           | torch.bfloat16 | torch.Size([2048])\n",
      "norm.scale                                         | torch.bfloat16 | torch.Size([2048])\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f531c69c97c12325"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
